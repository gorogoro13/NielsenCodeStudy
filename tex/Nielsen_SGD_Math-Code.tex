\documentclass[11pt,a4j,fleqn]{jarticle}  

%\usepackage[dvips]{graphicx}[dvipdfmx]
\usepackage[dvipdfmx]{graphicx}
%自動的に番号を振るマクロ 
\newcounter{apart}

\begin{document}

\title{
	\textbf{\Large  Michael Nielsen のニューラルネットワーク\\
	\bigskip
	「逆伝播法」の数式とソースコード}\\
	\bigskip
}

\bigskip
\bigskip
\author{なかじま　ひでき}
\date{\today}
\maketitle
%\renewcommand{\theapart}[1][]{\refstepcounter{apart}#1[Part{\theapart]：}
\renewcommand{\theapart}{\Alph{apart}}
\setcounter{apart}{1}

\textbf{\Large \theapart ：理論}
\refstepcounter{apart}

\section{コスト関数}
「逆伝播」はどのような誤差関数を採用するかが出発点になる。多クラス分類問題には二乗誤差関数を用いる（さらに学習精度を高めるためには交差エントロピー関数を用いるが、これは後回し）。
\subsection{定義}
\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2. \hspace{5mm}・・・「コスト関数 C 」と命名する。\nonumber \\
  実際に計算できるのは1つの訓練例xに対する誤差　\frac{1}{2} \sum_j (y_j-a^L_j)^2.　\nonumber 
\end{eqnarray}
　  w はネットワーク中の全ての重み、 b は全バイアス、 n は訓練入力の総数、y(x)は、入力xの時の目標出力、a は入力が x の時にネットワークから出力されるベクトル、和は全ての訓練入力 x を表す。\\
　この関数は、訓練入力 x の総和について表されている点が他の誤差関数と比べて特殊である。
\subsection{コスト関数の条件}
コスト関数の評価を通して逆伝播のコードを実装するにはいくつかの条件がある。実際に計算できるのはネットワーク全体ではなく（もしできたとしてもとてつもない計算負荷になる）、個々のニューロンだからだ。\\
定義から
\begin{itemize}
\item 総和（ネットワークの誤差）の中の全項目（ニューロンにおける誤差）が非負の値で示せること
\item C(w,b)$\simeq$0 の時は全訓練入力において 正解y(x) とテスト結果の出力aがほぼ等しくなる
\item 滑らかな関数であること（結局のところ分類の正解数を上げることが目的だが、分類の正解数がネットワーク中の重みとバイアスの滑らかな関数にならないのでコスト関数を導出する）
\end{itemize}
\subsection{コスト関数に(さらに)必要な2つの仮定}
・コスト関数は個々の訓練例xに対するコスト関数$C_x$の平均 $C = \frac{1}{n} \sum_x C_x$で書かれている。1つの訓練例に対するコスト関数を$C_x = \frac{1}{2} \|y-a^L \|^2$とすれば良いので 2乗コスト関数ではこの仮定が成立している。この仮定が必要となる理由は、逆伝播によって計算できるのは個々の訓練例に対する偏微分$\partial C_x / \partial w$、$\partial C_x / \partial b$だからです。\\
・コスト関数はニューラルネットワークの出力の関数で書かれている。\\
2乗コスト関数では1つの訓練例xに対する誤差は以下のように書ける。\\
\begin{eqnarray}
  C = C(a^L) = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2. \hspace{10mm} （27）\nonumber
\end{eqnarray}
----(以下　Chapter1の内容)--------------------------------------------------------------------------------\\
C($V_1$, $V_2$)としたときののグラフは次のようになる。
    \begin{center}
        \includegraphics[clip,width=8.5cm]{./valley.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
 $v_1$ 方向に微小な量 $\Delta v_1$ 、 v2 方向に微小な量 $\Delta v_2$ だけボールを動かした時に何が起こるか考えてみましょう。計算の結果、 Cの微小変化$\Delta C$は次のようになります:
\begin{eqnarray}
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
  \hspace{1.0cm} (２変数式の偏微分、３次元曲面上の近似解)
\end{eqnarray}
\\
{\large 問：『$ \Delta{C}$が負の値; すなわち、ボールが谷を転がり降りていくような $\Delta{V_1}$ と $\Delta{V_2}$ を選ぶ方法を見つけましょう。』}\\
 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ と定義して（教科書的なベクトルの行列表現に習って縦ベクトルにします）
\begin{eqnarray}
  \nabla C \equiv \left( \frac{\partial C}{\partial v_1},
  \frac{\partial C}{\partial v_2} \right)^T.
\end{eqnarray}
これまでの式からベクトル表現を使えば、
\begin{eqnarray}
  \Delta C \approx \nabla C \cdot \Delta v
\end{eqnarray}
η は小さい正のパラメータ(学習率として知られるもの)として、次の仮定を与えれば\\
\begin{eqnarray}
  \Delta v = -\eta \nabla C,
\end{eqnarray}

等式(9) から\\
\hspace{1.0cm} $ \Delta C \approx \nabla C \cdot \Delta v = \nabla C \cdot -\eta \nabla C =  -\eta \|\nabla C\|^2$
\hspace{1.0cm} ∴　 $\Delta C \approx -\eta \|\nabla C\|^2$ \\
$\|
\nabla C \|^2 \geq 0$ であることから $\Delta C \leq 0$ が成り立つため(10)の前提に従い vを変更する限り Cは常に減少し、決して増加しないことが保証されます(勿論(9) の等式が近似する限りです)。\\
  Cの微小変化$\Delta C$の符号を保証するために2乗の項を導き出した副産物が学習率$\eta$の意味を持ち他の副産物は発生しない完璧な変形です。\\
私たちは等式(10)を使い $\Delta v$の値を計算し、ボールの位置を v から次のように動かすのです:\\
\begin{eqnarray}
  v \rightarrow v' = v -\eta \nabla C.
\end{eqnarray}


勾配降下法の Cがちょうど二つの変数の関数である場合を説明しました。しかし、実際には、 Cが複数の関数であっても何も問題はありません。ここで Cが m個の変数 $v_1$,…,$v_m$の関数であると仮定します。この時、微小な変化 $\Delta v$ = (Δ$v_1$,…,Δ$v_m$)$^T$によってもたらされる Cの変化 $\Delta C$は\\
ｍ次元であったとしても同じ式で表すことができる。\\
\begin{itemize}
 \item  Cの微小変化\hspace{0.5cm} $ \Delta C \approx \nabla C \cdot \Delta v$
 \item  勾配 \hspace{0.5cm}  $\nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots \frac{\partial C}{\partial v_m}\right)^T$
 \item  学習率を仮定(あるいは２乗の項を導くために) \hspace{0.5cm} $\Delta v = -\eta \nabla C$
 \item  最小値への更新\hspace{0.5cm} $v \rightarrow v' = v-\eta \nabla C$
\end{itemize}
------------------------------------------------------------------------------------------------------\\
\newpage
\textbf{\Large \theapart ：コード}
\refstepcounter{apart}
\begin{verbatim}
 Nielsenの逆伝播アルゴリズムのソースコード "network.py"に定義されている関数backprop(self, x, y):のデバッグトレース
\end{verbatim}

\section{（関数定義部分の抜粋。コメント行は削除。）}
\begin{verbatim}
【source code】
    def backprop(self, x, y):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())

        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
 \end{verbatim}       
\section{ネットワークパラメータ修正量の初期化}
 \begin{verbatim}  
　ネットワークパラメータと同じShapeのndarrayを作成。
　        nabla_b = [np.zeros(b.shape) for b in self.biases]
        　nabla_w = [np.zeros(w.shape) for w in self.weights]
 \end{verbatim} 
 \subsection{「バイアス」修正量}
$ nabla_b …… list[array(30,1), (10,1)] ………B型と命名する$
 \subsection{「重み」修正量}
 $nabla_w ………… list[ array(30, 784), (10, 30) ]………W型と命名する$

\section{順伝播アルゴリズム実行時に保存しておく（複合）パラメータ} 
\begin{verbatim}
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
\end{verbatim}

\subsection{zs[l-1] (l層の出力直前層の重み付き出力) ………W型} 
$zs.Shape = list[array(30,784), (10,30)]$

\subsection{ activations[l-1]（activations[0]は入力データ）………B型}
$Shape.activations = list[array(30,784), (10,30)]$
\newpage
\section{出力層の逆伝播}
\begin{verbatim}
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
\end{verbatim}       
\subsection{nabla\_b…出力層の個々のニューロンの誤差=バイアスの偏微分=バイアスの修正量}

\verb|delta = self.cost_derivative(activations[-1], y) *  sigmoid_prime(zs[-1])| 
\verb|                ( return (activations - y ) ) * (return sigmoid(z)*(1-sigmoid(z)) )|
\verb|nabla b[-1] = delta|\hspace{5mm}$(\nabla b = {\delta}^{L}) $                                   
\begin{eqnarray}
{\delta}^{L}  = \nabla_a C \odot \sigma'(z^L) \hspace{5mm}(BP1 ベクトル表現) \nonumber \\
 \delta^L_j = \frac{\partial C}{\partial a^L_j}\sigma'(z^L_j).\hspace{5mm}(BP1定義) \nonumber
\end{eqnarray}
\subsection{nabla\_w…出力層Lの各ノードjが受け取る入力側(L-1)層のk個のノードの重みの修正量} 
\verb|nabla_w.Shape = (j, k)|になる。
\begin{verbatim}
nabla_w[-1] = np.dot(delta, activations[-2].transpose())
\end{verbatim}
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j. \hspace{5mm} (BP4)  \hspace{15mm}  \nonumber 
  \nabla w =  \delta^L \odot a^{L-1} \hspace{5mm} (BP4を出力層に適用)  \nonumber 
\end{eqnarray}
\begin{verbatim}
------------------------------------------------------
 delta ・ activations[-2].transpose() = nabla_w[-1]
(10, 1)・  (1, 30)             = (10, 30)のShapeになる。
------------------------------------------------------
 [[a]　・　[[a b c ...k個]]  = ([[ aa ab ac...k個],
  [b]                            [ ba bb bc...k個],
  [c]                            [ ca cb cc...k個],
  [d]                            [ ………………… ],
  [e]                            [ ………………… ],
  [f]                            [ ………………… ],
  [g]                            [ ………………… ],
  [h]                            [ ………………… ],
  [i]                            [ ………………… ],
  [j個]]                         [j個　　        ]])]
  ※print文で出力すると、カンマなしの出力になるが、float64の指数表記を扱った場合区切り文字がタブになるような挙動として理解。
\end{verbatim} 
\newpage
\section{中間層〜入力層の逆伝播}
\begin{verbatim}
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
        ------------------------------------------------------
z = zs[-l(エル)] （対象となる中間層の「重み付き出力」）
sp = sigmoid_prime(z)（対象となる中間層の「重み付き出力」におけるシグモイド関数の勾配）
     ( return sigmoid(z)*(1-sigmoid(z)) )
\end{verbatim} 
\hspace{10mm}$\sigma^{'}(z^l) = \sigma(z) × (1 - \sigma(z)  \hspace{5mm} (シグモイド関数の微分)$

\subsection{delta = np.dot(self.weights[-l+1].transpose(), delta) * sp}
(出力層では　\verb|delta = self.cost_derivative(activations[-1], y) *  sigmoid_prime(zs[-1])| )
\begin{eqnarray}
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l).  (BP2)\hspace{15mm}出力層の場合:\nabla_a C \odot \sigma'(z^L).\hspace{5mm} (BP1) \nonumber \\
\frac{\partial C}{\partial b^l_j} =  \delta^l_j. \hspace{5mm} (BP3) \hspace{115mm}  \nonumber 
\end{eqnarray}
\begin{eqnarray}
  \nabla b^l =  \delta^l  \hspace{5mm} (BP2,3からノード番号を省略したベクトル表現)  \nonumber 
\end{eqnarray}


\subsection{nabla\_w[-l] = np.dot(delta, activations[-l-1].transpose())} 
(出力層では　\verb| nabla_w[-1] = np.dot(delta, activations[-2].transpose())| )
\begin{eqnarray}
\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j  \hspace{5mm} (BP4)\hspace{15mm}出力層の場合:\delta^L \odot a^{L-1} \hspace{5mm} (BP4を出力層に適用)  \nonumber 
\end{eqnarray}
\begin{verbatim} 
4. 戻り値
（１）　nabla_b

（２）　nabla_w

\end{verbatim}
\end{document}