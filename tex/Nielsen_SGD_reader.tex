%LaTeX 数式の縦/横の調整
%http://www.math.kobe-u.ac.jp/~kodama/tips-latex-math-margin.html
\documentclass[11pt,a4paper,fleqn]{jsarticle}
\usepackage[dvipdfmx]{graphicx} 
\newcounter{apart}
%
% ######## measure #########
% # mm = 1mm = 2.85pt      #
% # cm = 10mm = 28.5pt     #
% # in = 25.4mm = 72.27pt  #
% # pt = 0.35mm = 1pt      #
% # em = width of [M]      #
% # ex = height of [x]     #
% # zw = width of [Kanji]  #
% # zh = height of [Kanji] #
% ##########################
% ##################### Portrait Setting #########################
% # TOP = 1inch + ¥voffset + ¥topmargin + ¥headheight + ¥headsep #
% #     = 1inch + 0pt + 4pt + 20pt + 18pt (default)              #
% # BOTTOM = ¥paperheight - TOP -¥textheight                     #
% ################################################################
\setlength{\textheight}{\paperheight}   % 紙面縦幅を本文領域にする（BOTTOM=-TOP）
\setlength{\topmargin}{4.6truemm}       % 上の余白を30mm（=1inch+4.6mm）に
\addtolength{\topmargin}{-\headheight}  % 
\addtolength{\topmargin}{-\headsep}     % ヘッダの分だけ本文領域を移動させる
\addtolength{\textheight}{-60truemm}    % 下の余白も30mm（BOTTOM=-TOPだから+TOP+30mm）
% #################### Landscape Setting #######################
% # LEFT = 1inch + ¥hoffset + ¥oddsidemargin (¥evensidemargin) #
% #      = 1inch + 0pt + 0pt                                   #
% # RIGHT = ¥paperwidth - LEFT - ¥textwidth                    #
% ##############################################################
\setlength{\textwidth}{\paperwidth}     % 紙面横幅を本文領域にする（RIGHT=-LEFT）
\setlength{\oddsidemargin}{-0.4truemm}  % 左の余白を25mm(=1inch-0.4mm)に
\setlength{\evensidemargin}{-0.4truemm} % 
\addtolength{\textwidth}{-50truemm}     % 右の余白も25mm（RIGHT=-LEFT）
%
\title{Michael Nielsenの「ニューラルネットワークと深層学習」数学的読解}
\author{なかじま　ひでき}
\date{\today}


\begin{document}
\maketitle
\renewcommand{\theapart}{\Alph{apart}}
\setcounter{apart}{1}
%
%

\begin{verbatim}
【原文（日本語訳）】http://nnadl-ja.github.io/nnadl_site_ja/index.html
【原文（英語）】http://neuralnetworksanddeeplearning.com/
【キーワード】　シグモイド関数　多変数式の微分（偏微分）
　----------------------------------------------------------------------------------------------------
\end{verbatim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{ \: CHAPTER 1　ニューラルネットワークを用いた手書き文字認識}
%\maketitle
\textbf{\Large \theapart : CHAPTER 1 ニューラルネットワークを用いた手書き文字認識}
\refstepcounter{apart}
\section{学習データ}
\section{順伝播ネットワーク}
%\subsection{活性化(activation)関数}

\subsection{活性化関数としてのシグモイド関数}
\setcounter{equation}{2}
\begin{equation}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\end{equation}
『より明確に表現すると、シグモイドニューロンの出力は、入力が$x_1$,$x_2$,…で、重みが$w_1$,$w_2$,…で、そしてバイアスがbのとき、次の形を取ります。』
\begin{eqnarray}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}
\hspace{1.0cm} {\large ∵}  $\sigma(z) \equiv \frac{1}{1+e^{-z}}　= \frac{1}{1 + \exp{(-z)}} \hspace{1.0cm}$
$z = \sum_{j} w_j x_j -b$\\
    \begin{center}
        \includegraphics[clip,width=8.5cm]{./sigmoid.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
    
◆ $\exp{(x)}$は指数関数(exponential function) $e^x$を表す。 \\
指数函数は指数法則と呼ばれる基本的な関係式
\begin{eqnarray}
  \exp{(x + y)} = \exp{(x)} \exp{(y)} \hspace{2.0cm} ∵ e^{(x + y)} = e^x e^y \nonumber
\end{eqnarray}


を満たすから、指数函数を冪乗の記法を以って $e^x$ と書くこともある。\\
◆ eは、ネイピア数（Napier's constant） 自然対数の底。e = 2.71828 18284 59045 23536 02874 71352 …
と続く超越数である。欧米ではオイラー数 (Euler's number) と呼ばれることもあるが、オイラーの定数 γ やオイラー数列とは異なる。

 $e = \lim_{n \to \infty} \left( 1 + \frac{1}{n}  \right)^n$\hspace{1.0cm}
 $\exp{(x)} = \lim_{n \to \infty} \left(  1 + \frac{x}{n}  \right)^n $
\newpage
\textbf{\Large \theapart ：CHAPTER2 逆伝播の仕組み}
\refstepcounter{apart}

\section{逆伝播ネットワーク…「勾配降下法」}
このセクションでは、「コスト関数の勾配をどのように計算するか」見ていきます。アルゴリズム的には{\large \bf 「逆伝播」}と呼ばれる、コスト関数の勾配を高速に計算するアルゴリズムを説明します。\\
逆伝播の本質はコスト関数Cのネットワークの重みw（もしくはバイアスb）に関する偏微分$\partial C / \partial w$($\partial C / \partial b$) （∂C/∂b∂C/∂b）です。 偏微分をみると、重みとバイアスを変化させた時のコスト関数の変化の度合いがわかります。
\subsection{ニューラルネットワーク中の重みを曖昧性なく指定する表記方法}
$w^l_{jk}$ で(l−1)番目の層のk番目のニューロンからl番目の層のj番目のニューロンへの接続に対する重みを表します。 例えば、図１は2番目の層の4番目のニューロンから3番目の層の2番目のニューロンへの接続の重みを表します。この表記方法で若干曲者なのが、jとkの順番です。 jを入力ニューロン、kを出力ニューロンとする方が理にかなっていると思うかもしれませんが、 実際には逆にしています。 奇妙なこの記述方法の理由は後程説明します。

\begin{figure}[htbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=150mm]{./tikz16.eps}
  \end{center}
  \caption{重み(何様度数) $w^l_{jk}$}
 % \label{fig:two}
 \end{minipage}
\end{figure} 

 \hspace{2mm} \verb|数学添字 l : レイヤー番号。入力層を１として出力側に向かって１，２と増える。| \\
 \hspace{5mm} \verb|数学添字 j : l番レイヤーのj番のニューロン |\\
 \hspace{5mm} \verb|数学添字 k : (l-1)番レイヤーのk番のニューロン |\\

\newpage
\subsection{バイアスと活性を指定する表記方法}

\begin{figure}[htbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=65mm]{./tikz17.eps}
  \end{center}
  \caption{バイアス(俺様度数) $b^l_{j}$}
 % \label{fig:one}
 \end{minipage}
\end{figure}
 \hspace{2mm} $b^l_j$\verb|でl番目の層のj番目のニューロンのバイアス|\\
 \hspace{5mm} $a^l_j$\verb|でl番目の層のj番目のニューロンの活性を表します。|
\subsection{ベクトル表現}
 \begin{eqnarray}
 l番目の層のj番目のニューロンの活性 a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right). \nonumber \hspace{10mm}(23)
\end{eqnarray}
この式を行列で書き直すため、各層lに対し重み行列$w^l$(ベクトルではなく行列)を定義します。\\
重み行列$w^l$の各要素はl番目の層のニューロンを終点とする接続の重みです。 \\
すなわち、j行目k列目の要素を$w^l_{jk}$とします。\\
・重み行列$w^l$　　　　　…　$w^l_{jk}$　　同様に、\\
・バイアスベクトル$b^l$　…　$b^l_j$ \\
・活性ベクトル$a^l$　　　…　$a^l_j$ \\
(23) を行列形式に書き直すのに必要な最後の要素は、σなどの関数のベクトル化です。σのような関数をベクトルvの各要素に適用したいというのがアイデアです。このような各要素への関数適用には$σ(v)$という自然な表記を用います。 つまり、$σ(v)$の各要素は$σ(v)_j=σ(v_j)$です。 例えば$f(x)=x^2$とすると、次のようになります。
\begin{eqnarray}
  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right]. \hspace{10mm}(24) \nonumber
\end{eqnarray}
式 (23) は
\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l). \hspace{10mm}(25) \nonumber
\end{eqnarray}
ニューロン単位での見方よりも簡潔で、添字も少なくて済みます。$a^l$の計算のために式 (25) を利用する時には、途中で$z^l \equiv w^l a^{l-1}+b^l$を計算しています。\\
・重みつき入力$z^l \equiv w^l a^{l-1}+b^l$ \\
・$z^l$の要素は$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$ \\
 つまり、$z^l_j$はl番目の層のj番目のニューロンが持つ活性関数へ与える重みつき入力です。
\subsection{コスト関数}
\subsubsection{コスト関数の条件}
コスト関数は正解と計算結果の誤差を表す指標である。逆伝播が機能するには、コスト関数の形について2つの仮定を置く必要があります。コスト関数の評価を通して逆伝播のコードを実装するにはいくつかの条件がある。実際に計算できるのはネットワーク全体ではなく（もしできたとしてもとてつもない計算負荷になる）、個々のニューロンだからだ。\\
CHAPTER1 式（６）の定義
\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2. \nonumber
\end{eqnarray}
から
\begin{itemize}



\item 総和（ネットワークの誤差）の中の全項目（ニューロンにおける誤差）が非負の値で示せること
\item C(w,b)$\simeq$0 の時は全訓練入力において 正解y(x) とテスト結果の出力aがほぼ等しくなる
\item 滑らかな関数であること（結局のところ分類の正解数を上げることが目的だが、分類の正解数がネットワーク中の重みとバイアスの滑らかな関数にならないのでコスト関数を導出する）
\end{itemize}
\subsubsection{コスト関数 [平均二乗誤差　MSE(mean squared error)]}
\setcounter{equation}{5}
\begin{eqnarray}  コスト関数 [平均二乗誤差MSE(mean squared error)] \hspace{2mm}C(w,b) \equiv
   \frac{1}{2n} \sum_x \| y(x) - a\|^2. \hspace{5mm}(26) \nonumber
\end{eqnarray}
nは訓練例の総数、訓練例xについてy(x)は正解、$a^L(x)$はニューラルネットワークの出力のベクトルです。
\subsubsection{コスト関数に(さらに)必要な2つの仮定}
・コスト関数は個々の訓練例xに対するコスト関数$C_x$の平均 $C = \frac{1}{n} \sum_x C_x$で書かれている。1つの訓練例に対するコスト関数を$C_x = \frac{1}{2} \|y-a^L \|^2$とすれば良いので 2乗コスト関数ではこの仮定が成立している。この仮定が必要となる理由は、逆伝播によって計算できるのは個々の訓練例に対する偏微分$\partial C_x / \partial w$、$\partial C_x / \partial b$だからです。\\
・コスト関数はニューラルネットワークの出力の関数で書かれている。\\
2乗コスト関数では1つの訓練例xに対する誤差は以下のように書ける。\\
\begin{eqnarray}
  C = C(a^L) = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2. \hspace{10mm} （27）\nonumber
\end{eqnarray}
----(以下　Chapter1の内容)--------------------------------------------------------------------------------\\
C($V_1$, $V_2$)としたときののグラフは次のようになる。
    \begin{center}
        \includegraphics[clip,width=8.5cm]{./valley.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
 $v_1$ 方向に微小な量 $\Delta v_1$ 、 v2 方向に微小な量 $\Delta v_2$ だけボールを動かした時に何が起こるか考えてみましょう。計算の結果、 Cの微小変化$\Delta C$は次のようになります:
\begin{eqnarray}
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
  \hspace{1.0cm} (２変数式の偏微分、３次元曲面上の近似解)
\end{eqnarray}
\\
{\large 問：『$ \Delta{C}$が負の値; すなわち、ボールが谷を転がり降りていくような $\Delta{V_1}$ と $\Delta{V_2}$ を選ぶ方法を見つけましょう。』}\\
 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ と定義して（教科書的なベクトルの行列表現に習って縦ベクトルにします）
\begin{eqnarray}
  \nabla C \equiv \left( \frac{\partial C}{\partial v_1},
  \frac{\partial C}{\partial v_2} \right)^T.
\end{eqnarray}
これまでの式からベクトル表現を使えば、
\begin{eqnarray}
  \Delta C \approx \nabla C \cdot \Delta v
\end{eqnarray}
η は小さい正のパラメータ(学習率として知られるもの)として、次の仮定を与えれば\\
\begin{eqnarray}
  \Delta v = -\eta \nabla C,
\end{eqnarray}

等式(9) から\\
\hspace{1.0cm} $ \Delta C \approx \nabla C \cdot \Delta v = \nabla C \cdot -\eta \nabla C =  -\eta \|\nabla C\|^2$
\hspace{1.0cm} ∴　 $\Delta C \approx -\eta \|\nabla C\|^2$ \\
$\|
\nabla C \|^2 \geq 0$ であることから $\Delta C \leq 0$ が成り立つため(10)の前提に従い vを変更する限り Cは常に減少し、決して増加しないことが保証されます(勿論(9) の等式が近似する限りです)。\\
  Cの微小変化$\Delta C$の符号を保証するために2乗の項を導き出した副産物が学習率$\eta$の意味を持ち他の副産物は発生しない完璧な変形です。\\
私たちは等式(10)を使い $\Delta v$の値を計算し、ボールの位置を v から次のように動かすのです:\\
\begin{eqnarray}
  v \rightarrow v' = v -\eta \nabla C.
\end{eqnarray}


勾配降下法の Cがちょうど二つの変数の関数である場合を説明しました。しかし、実際には、 Cが複数の関数であっても何も問題はありません。ここで Cが m個の変数 $v_1$,…,$v_m$の関数であると仮定します。この時、微小な変化 $\Delta v$ = (Δ$v_1$,…,Δ$v_m$)$^T$によってもたらされる Cの変化 $\Delta C$は\\
ｍ次元であったとしても同じ式で表すことができる。\\
\begin{itemize}
 \item  Cの微小変化\hspace{0.5cm} $ \Delta C \approx \nabla C \cdot \Delta v$
 \item  勾配 \hspace{0.5cm}  $\nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots \frac{\partial C}{\partial v_m}\right)^T$
 \item  学習率を仮定(あるいは２乗の項を導くために) \hspace{0.5cm} $\Delta v = -\eta \nabla C$
 \item  最小値への更新\hspace{0.5cm} $v \rightarrow v' = v-\eta \nabla C$
\end{itemize}
------------------------------------------------------------------------------------------------------\\
\subsection{勾配降下法}
この規則は勾配降下法の定義とみなすことができます。\\
これは $\Delta C \cdot \Delta v$を最小化する $\Delta v$が $\Delta v$=−η$\nabla C$であり η=$\epsilon$ / $\|\nabla C\|$は制約量 $\|\Delta v\|$ = $\epsilon$ により決まるということから証明できます。つまり、勾配降下法はその瞬間に Cを最も減少させる方向へと小刻みに動く方法と見なすことができます。\\
\subsection{「確率的勾配降下法」---勾配降下法の欠点と改善}
\subsubsection{勾配降下法の欠点}
大きな欠点も持っています：二階偏微分の計算が必要であり、その計算が非常に大変なのです。なぜ計算が大変か明らかにするため、全ての二階偏微分 $\partial^2 C/\partial v_j \partial v_k$を計算したいと仮定してみましょう。仮に100万の変数がある時、私たちはおよそ1兆回(つまり100万の2乗)の二階微分の計算が必要で計算量的に重くなります。
(実際は、1兆の半分、なぜなら $\partial^2 C/\partial v_j\partial v_k$=$\partial^2 C/\partial v_k\partial v_j$だからです。)\\
\subsubsection{勾配降下法の再定義}
変数 $v_j$を重みとバイアスに置き換えて勾配降下法の更新規則を再定義しましょう。つまり、私たちの"位置"は要素として $w_k$と $b_l$を持っており、勾配ベクトル $\nabla C$は要素として $\partial C/\partial w_k$と
$\partial C/\partial b_l$を持っていることに一致します。これらの要素の用語で勾配降下法の更新規則を書き直すと、
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\end{eqnarray}
\subsubsection{勾配降下法のいくつかの課題}
今は一つの問題にだけ言及したいと思います。問題が何であるかを理解するため、等式(6)の二次コスト関数を振り返りましょう。ここでコスト関数は \\
C=$\frac{1}{n}$ $\sum_x \nabla C_x$という形をしており、\\
個々の訓練データ $C_x$≡$\frac{\|y(x)−a\|^2}{2}$の総和になっていることが分かると思います。\\
実際には、私たちは勾配$\nabla C$を計算するため、個々の訓練入力 xの勾配\\
$\nabla C_x$を計算し、その後その平均を取って  \\
$\nabla C$ = $\frac{1}{n} $ $\sum_x \nabla C_x$ とします。\\
不運にも、訓練入力の数が非常に大きい場合はとても時間が掛かり、その結果学習は非常に遅くなってしまいます。
\subsubsection{確率的勾配降下法}
ランダムに選んだ訓練入力を $X_1$,$X_2$,…,$X_m$とラベル付けし、これらを{\gt \large ミニバッチ}と呼ぶことにしましょう。標本サイズ m が十分大きければ $\nabla C_{X_j}$ の平均値は全ての $\nabla C_x$の平均とほぼ同等になることが期待でき、すなわち、
\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\end{eqnarray}
ここで二つ目の総和は全ての訓練データです。端と入れ替えると、
\begin{eqnarray}
  \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},
\end{eqnarray}
ランダムに選んだミニバッチを計算して全体の勾配を推定できることが確認できます。\\
\\
ニューラルネットワークにおける重みとバイアスの表記 $w_k$と $b_l$で考えてみましょう。
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial b_l},
\end{eqnarray}
総和は現在のミニバッチにおける全ての訓練サンプル $X_j$です。次に、私たちは別の無作為に選んだミニバッチで訓練を行います。同じように、訓練入力がなくなるまで続ければ、画期的な訓練の完成です。この時点で私たちは新しい画期的な方法を使って訓練をやり直します。\\
\\
ちなみに、コスト関数とミニバッチにおける重みとバイアスの更新尺度の多様な慣習は注目に値します。等式(6)で私たちは全てのコスト関数を $\frac{1}{n}$の縮尺にしました。しばしば人々は $\frac{1}{n}$を省略し、個々の訓練例のコストの平均を取る代わりに総和を取ります。これはとりわけ訓練例の総数が事前に分かっていない場合に有効です。これは、例えば、リアルタイムに訓練データが生成されている場合に生じ得ます。そして、同じように、ミニバッチの更新規則 (20) と (21) でもしばしば総和の前にある $\frac{1}{m}$ を省略します。これは学習率 ηの縮尺の大きさを変更することに相当するので概念的には大差がありません。しかし、両者の動作の詳細な比較について気にすることは価値のあることです。

◆---------行列式とベクトル式の意味がわからん-----------------------------------------\\
\verb|＞[1]|　2×3の行列と表現した時は2行３列の行列ということの短縮形だが、具体的には\\
\[
  A = \left(
    \begin{array}{ccc}
      a & b & c \\
      d & e & f 
    \end{array}
  \right)
\]
の形をしている行列のことだ。要素数で行列の２×３を解釈すると各行には３要素あるので３×２と順番が入れ替わるので間違いやすい。\\
＞\verb|[2]　$\nabla$は文脈によって使い分けの多い記号だが、ここでは「勾配(gradient)」と呼ばれている定義によって使用されている。|\\
スカラー場 f のベクトル微分は勾配(en:gradient)と呼ばれ、\\
\begin{eqnarray}
\nabla f = \frac{\partial f}{\partial x}\hat{x} + \frac{\partial f}{\partial y}\hat{y} + \frac{\partial f}{\partial z}\hat{z} \nonumber
\end{eqnarray}
と表せる。\\
この記法が特に強力なのは、一次元の場合の微分と同様の積の規則\\
$\nabla (fg) = f\nabla g + g\nabla f$\\
が成り立つことにある。\\
\verb|詳しくはWikipediaや,http://hyropom.web.fc2.com/phys/nabla-0815-.pdf を参照|\\
＞\verb|[3]　内積　ヘクトルの乗算|\\
行数と列数が演算可能なように解釈すれば良い。転置は書き方の問題で、特に問題にならない場面では律儀に行列の縦横にはこだわらない。数学者にとっては、どうでもいい行列の内容表記に何行も使う縦ベクトルはうざいのだ。
\verb|参照：http://www.geisya.or.jp/~mwm48961/linear_algebra/simple_example.htm|
-----------------------------------------------------------------------------------------\\
\section{「10進数字」クラス分類の実装}
\subsection{簡単な例}
net = Network([2, 3, 1])　と初期化したネットワークについて考える。\\
$w^l_{jk}$ … (l−1)番目の層のk番目のニューロンからl番目の層のj番目のニューロンへの接続に対する「重み」\\
\\
third layer of neurons is:
\begin{eqnarray}
  a' = \sigma(w a + b).
\end{eqnarray}
・・・本文英語のみ。コーデイングの解説\\
・・・（省略）・・・\\
-------------------------------------------------------------------------------------------\\
\\
\newpage

\title{CHAPTER 2 逆伝播の仕組み}
\maketitle
\section{簡単な例}
\subsection{ニューラルネットワーク中の重みを曖昧性なく指定する表記方法}
\subsubsection{「重み」}
net = Network([3, 4, 2])　と初期化したネットワークについて、

    \begin{center}
        \includegraphics[clip,width=13cm]{./tikz16.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
$w^l_{jk}$は(l−1)番目の層のk番目のニューロンからl番目の層のj番目のニューロンへの接続に対する「重み」\\
\subsubsection{「バイアス」と「活性」}
    \begin{center}
        \includegraphics[clip,width=6cm]{./tikz17.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
$b^l_j$でl番目の層のj番目のニューロンのバイアスを表す\\
$a^l_j$でl番目の層のj番目のニューロンの活性をす\\
\begin{eqnarray}
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right).
\end{eqnarray}
\newpage
\subsubsection{活性関数のベクトル化…「重み行列」と「バイアスベクトル」で活性を計算する}
σのような関数をベクトルvの各要素に適用したい\\
$\sigma(v)$の各要素は$\sigma(v)_j = \sigma(v_j)$です。 例えば$f(x) = x^2$とすると、次のようになります。
\begin{eqnarray}
  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right].
\end{eqnarray}
この表記方法を用いると、式 (23) は次のような美しくコンパクトなベクトル形式で書けます。
\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l)
\end{eqnarray}
 式 (25) をしばしば{\large 重み付き入力} $z^l$を用いて$a^l = \sigma(z^l)$とも書きます。\\
$z^l$の要素は$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$と書ける事にも注意してください。 つまり、$z^l_j$はl番目の層のj番目のニューロンが持つ活性関数へ与える重みつき入力です。
\\
\section{コスト関数に必要な２つの仮定}
逆伝播が機能するには、コスト関数の形について2つの仮定を置く必要があります。\\
2乗コスト関数（参考：式(6)）を考えます。2乗コスト関数は以下の様な形をしていました
\begin{eqnarray}
  C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2.
\end{eqnarray}
nは訓練例の総数、和は個々の訓練例xについて足しあわせたもの、$y = y(x)$は対応する目標の出力、Lはニューラルネットワークの層数、$a^L = a^L(x)$はxを入力した時のニューラルネットワークの出力のベクトルです。
\subsection{1つ目の仮定}
コスト関数は個々の訓練例xに対するコスト関数$C_x$の平均 $C = \frac{1}{n} \sum_x C_x$で書かれている\\
逆伝播によって計算できるのは個々の訓練例に対する偏微分$\partial C_x / \partial w$、$\partial C_x / \partial b$だからです。 コスト関数の偏微分$\partial C_x / \partial w$、$\partial C_x / \partial b$は全訓練例についての平均を取ることで得られます。 この仮定を念頭に置き、私達は訓練例xxを1つ固定していると仮定し、コストCxCxを添字xxを除いてCCと書くことにします。
\newpage

\subsection{2つ目の仮定}
コスト関数はニューラルネットワークの出力の関数で書かれている
    \begin{center}
        \includegraphics[clip,width=10cm]{./tikz18.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
\begin{eqnarray}
  C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2.
\end{eqnarray}
C を出力の活性$a^L$単独の関数とみなし、yは関数を定義するための単なるパラメータとみなすのは意味のある問題設定です。\\

◆(予習)--------アダマール積$s \odot t$--------------------------------------------------------\\
$s \odot t$を2つのベクトルの要素ごとの積とします。
\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 2 \end{array}\right]
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right]
\end{eqnarray}
---------------------------------------------------------------------------------------\\
\section{逆伝播の基礎となる4つの式}
逆伝播は重みとバイアスの値を変えた時にコスト関数がどのように変化するかを把握する方法です。 これは究極的には\\
\begin{eqnarray}
 \frac{\partial C}{\partial w^l_{jk}} \nonumber \hspace{5mm},\hspace{5mm}%∂C/∂wljk∂C/∂wjkl \\
\frac{\partial C}{\partial b^l_j}  \nonumber %∂C/∂blj∂C/∂bjl
\end{eqnarray}
を計算する事を意味します。これらの偏微分を計算する為にまずは中間的な値$\delta^l_j$を導入します。\\
l番目の層のjj番目のニューロンの誤差 = $\delta^l_j$ \\
\\
{\Large \bf ◆「こびと」$\Delta z^l_j$　} \\
l番目の層のj番目のニューロンに「小人」がいて、\\
{\Large \bf 重みつき入力$z^l_j$ (= $w^l_{jk} * a^{(l-1)}_k + b^l_j$) }に小さな変更$\Delta z^l_j$を加える。\\
----（補足）-------------------------------------------------------------------------------------\\

「重み付き入力」の定義：
$z^l \equiv w^l a^{l-1}+b^l$\\
----(引用)--------------------------------------------------------------------------------
\begin{eqnarray}
  l番レイヤーの出力ベクトル \hspace{0.5mm}a^{l} = \sigma(w^l a^{l-1}+b^l).\hspace{10mm}(25) \nonumber
\end{eqnarray}
この表現を用いると、ある層の活性とその前の層の活性との関係を俯瞰できます。 我々が行っているのは活性に対し重み行列を掛け、バイアスベクトルを足し、最後にσ関数を適用するだけです。この見方はこれまでのニューロン単位での見方よりも簡潔で、添字も少なくて済みます。 議論の正確性を失う事なく添字地獄から抜け出せる方法と考えると良いでしょう。 さらに、この表現方法は実用上も有用です。 というのも、多くの行列ライブラリでは高速な行列掛算・ベクトル足し算・関数のベクトル化の実装が提供されているからです。 実際、前章のコード では、ネットワークの挙動の計算にこの表式を暗に利用していました。\\
\\
( *ところで、先ほどの$w^l_{jk}$という奇妙な表記を用いる動機はこの式に由来します。 もし、jを入力ニューロンに用い、kを出力ニューロンに用いたとすると、式 (25) は重み行列をそれの転置行列に置き換えなければなりません。 些細な変更ですが、煩わしい上に「重み行列を掛ける」と簡単に言ったり（もしくは考えたり）できなくなってしまいます。 )\\
\\
$a^l$の計算のために式 (25) を利用する時には、途中で$z^l \equiv w^l a^{l-1}+b^l$を計算しています。 この値は後の議論で有用なので名前をつけておく価値があります。 $z^l$をl番目の層に対する{\Large \bf 重みつき入力}と呼ぶことにします。 本章の以降の議論では重みつき入力$z^l$を頻繁に利用します。 \\
{\large \bf 式 (25) をしばしば重み付き入力を用いて$a^l = \sigma(z^l)$とも書きます。 $z^l$の要素は$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$と書ける事にも注意してください。} \\
つまり、$z^l_j$はl番目の層のj番目のニューロンが持つ活性関数へ与える重みつき入力です。\\
----------------------------------------------------------------------------------------（引用おわり）-------\\

「重み」は$w^l_{jk}$という行列表現になるので添字を追いかけるのに苦労する。「重み付き入力」は、当該レイヤー$l$のj番目ニューロンの「重み付き入力」であり話を簡単にできる。「重み」に戻らなければならないのは、ネットワークモデルの重みを学習によって更新するときである。\\
-----------------------------------------------------------------------------------------\\
このニューロンは、$\sigma {(z^l_j)}$の代わりに$\sigma {(z^l_j + \Delta z^l_j)}$を出力する。この変化はニューラルネット中の後段の層に伝播し、最終的に全体のコスト関数の値は\\
\begin{eqnarray}
 \frac{\partial C}{\partial z^l_j}\Delta z^l_j \nonumber
\end{eqnarray}
だけ変化します。コストを小さくするような$\Delta z^l_j$を加えるのが「小人」の使命です。\\
 $\frac{\partial C}{\partial z^l_j}$が大きな値（正でも負も良いです）であるとします。 すると、$\frac{\partial C}{\partial z^l_j}$と逆の符号の$\Delta z^l_j$を選ぶことで、この悪魔はコストをかなり改善させられます。 逆に、もし$\frac{\partial C}{\partial z^l_j}$が0に近いと悪魔は重みつき入力$z^l_j$を摂動させてもコストをそれほどは改善できません。 悪魔が判断できる範囲においてはニューロンは既に最適に近い状態だと言えます(※)  つまり、ヒューリスティックには、$\frac{\partial C}{\partial z^l_j}$はニューラルネットワークの誤差を測定しているという意味を与える事ができます。\\
 ※　もちろんこれが正しいのは$\Delta z^l_j$が小さい場合に限ってです。小人は微小な変化しか起こせないと仮定しています。\\
 \\
 l番目の層のj番目のニューロンの誤差$\delta ^l_j$を以下のように定義します
\begin{eqnarray}
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}. \nonumber
\end{eqnarray}
小人が$\Delta z^l_j$を加えることによる最終的なコスト関数の変化は
\begin{eqnarray}
 \frac{\partial C}{\partial z^l_j}\Delta z^l_j \nonumber
\end{eqnarray}
でした。
\subsection{ニューロンの誤差}
l番目の層のj番目のニューロンの誤差$\delta^l_j$を以下のように定義します
\begin{eqnarray}
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.
\end{eqnarray}
\subsection{出力層での誤差$\delta^L$　【BP1】}
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j).  \hspace{4.0cm}【BP1】 \nonumber
\end{eqnarray}
\begin{eqnarray}
  \delta^L = \nabla_a C \odot \sigma'(z^L).    \hspace{4.0cm}【BP1a】 \nonumber
\end{eqnarray}
\begin{eqnarray}
  \delta^L = (a^L-y) \odot \sigma'(z^L). \hspace{20mm}(30) \nonumber
\end{eqnarray}
\subsection{誤差$\delta^{l}$の次層での誤差$\delta^{l+1}$【BP2】}
\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l).    \hspace{4.0cm}【BP2】 \nonumber
\end{eqnarray}
(BP2) を (BP1) と組み合わせる事で、ニューラルネットワークの任意の層lでの誤差$\delta^l$を計算できます。 まず、$\delta^L$を式 (BP1) で計算します。 次に、式 (BP2) を適用して$\delta^{L-1}$を計算します。 その後、再び (BP2) を適用して、$\delta^{L-2}$を計算します。以下これを繰り返してニューラルネットワークを逆向きに辿る事ができます。
\subsection{任意のバイアスに関するコストの変化率【BP3】}
\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =  \delta^l_j.    \hspace{4.0cm}【BP3】 \nonumber
\end{eqnarray}
誤差$\delta^l_j$はコスト関数の変化率$\partial C / \partial b^l_j$と完全に同一です。簡潔に
\begin{eqnarray}
  \frac{\partial C}{\partial b} = \delta
\end{eqnarray}
と書くことができます。
\subsection{任意の重みについてのコストの変化率【BP4】}
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.    \hspace{4.0cm}【BP4】 \nonumber
\end{eqnarray}
偏微分$\partial C / \partial w^l_{jk}$を計算方法が既知の$\delta^l$と$a^{l-1}$を用いて計算できることがわかります。 この式はもう少し添字の軽い式で
\begin{eqnarray}  \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out},
\end{eqnarray}
と書き直せます\\
    \begin{center}
        \includegraphics[clip,width=5cm]{./tikz20.png} \\
        %\hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
\newpage
\section{4つの式の証明}
これらはすべて多変数関数の微分の連鎖律を用いて証明できます。\\
{\Large \bf 【連鎖律(多変数関数の合成関数の微分)】}\verb|http://mathtrain.jp/rensaritsu| 
\begin{eqnarray}
\frac{\partial f}{\partial x}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial x} \nonumber
\end{eqnarray}
\begin{eqnarray}
\frac{\partial f}{\partial y}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial y} \nonumber
\end{eqnarray}
\subsection{出力層での誤差(BP1)}
\setcounter{equation}{35}
【BP1】は次の形をしている。\\
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j). \nonumber
\end{eqnarray}
この式に至るために小人の話をして、l番目の層のj番目のニューロンの{\Large \bf 誤差$\delta^l_j$}を以下のように定義しました・
\begin{eqnarray}
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.　\nonumber
\end{eqnarray}
l=L(出力層)に限定すると
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial z^L_j}.    \hspace{4.0cm}です。 
\end{eqnarray}
連鎖律を適用すると、この微分を出力活性に関する偏微分で書ける。
\begin{eqnarray}
  \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j}.\hspace{2.0cm}(kは前層のニューロン番号)
\end{eqnarray}
和は出力層のすべてのニューロンkについて足し合わせます。\\
(ここまではよいとして)\\
k=jの時には、k番目のニューロンの出力活性$a^L_k$は、j番目のニューロンの入力の重み（※１）にのみ依存します。\\
【原文】Of course, the output activation $a^L_j$ of the $k^{th}$ neuron {\large \bf depends only on the input weight $z^L_j$ for the $j^{th}$ neuron} when k=j.\\
【和訳】 もちろん、k=jの時には、k番目のニューロンの出力活性$a^L_j$は、{\large \bf j番目のニューロンの入力の重みにのみ依存します。}\\
【改良和訳】 もちろん、k=jの時には、k番目のニューロンの出力活性$a^L_j$は、{\large \bf j番目のニューロンの入力の「重み付き入力$z^L_j$」にのみ依存します。}\\
\begin{eqnarray}
従って、k≠jの時には\hspace{5mm}  \frac{\partial a^L_k}{\partial z^L_j}\hspace{5mm} の値は0です。（※２） \nonumber
\end{eqnarray}
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.
\end{eqnarray}
$a^L_j = \sigma(z^L_j)$であった事を思い出すと、第2項は$\sigma'(z^L_j)$と書けて、
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)
\end{eqnarray}
これを添字なしの形式で書くと (BP1) が得られます。\\
※１，２------------------------------------------------------------------------------------------\\
 \hspace{2mm} 数学添字 l : レイヤー番号。入力層を１として出力側に向かって１，２と増える。 \\
 \hspace{5mm} 数学添字 j : l番レイヤーのj番のニューロン \\
 \hspace{5mm} 数学添字 k : (l-1)番レイヤーのk番のニューロン \\
 --------------------------------------------------------------------------------の規則に従うと\\
 （※１）k=jの時には、k番目のニューロンの出力活性$a^L_k$は、j番目のニューロンの入力の重みは、\\
 $w^L_{j=k,k=1,2...k}$ の重みベクトルのことを指す。\\
 \\
 （※２）混乱するのは、
 \begin{eqnarray}
「k≠jの時には\hspace{5mm}\frac{\partial a^L_k}{\partial z^L_j}
\hspace{5mm} の値は0です。」\nonumber 
\end{eqnarray}
という表現の「$k=j$」は「$j≠k$」にすべきで、$j≠k$の時は出力層Lのk番目以外のニューロンに適用される重みということになる。配列を用いたアルゴリズム的には明白だが、数学表現の場合は「相殺される」とか「０（ゼロ）である」ということになる。\\
 　あるいは、逆伝播の時には出力レイヤーを開始点として入力レイヤーに向かって「順伝播」すると考えてkとjが入れ替わるというのが数学的な流儀に従った解釈と言える。従って、\\
 \begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_k}.\hspace{5mm}は\hspace{5mm}\delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_{j=k}} \nonumber
\end{eqnarray}
$a^L_j = \sigma(z^L_j)$であった事を思い出すと、第2項は$\sigma'(z^L_j)$と書けて、
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber
\end{eqnarray}

\end{document}

        \includegraphics[clip,width=8.5cm]{./tikz16.png} \\
        \hspace{1.0cm}【C($V_1$, $V_2$)のグラフ】
    \end{center}
